D1 = [0.25, 0.25, 0.25, 0.25]
D2 = [0.5, 0.25, 0.25, 0]

Entropy: Ask how many binary question to know which class
+ Sigma item
+ item = prob * tree_height
+ tree_height = 1/prob
-> E = sigma -p*log(p)

C1 = -0.25*log(0.25) -0.25*log(0.25) -0.25*log(0.25) -0.25*log(0.25) = 2
C2 = -0.5*log(0.5) -0.25*log(0.25) -0.25*log(0.25) -0*log(0) = 1.5

Binary classification entropy:
D = [p, 1-p]
BE = p*log(p) + (1-p)*log(1-p)

Binary cross entropy: Difference from D1 to D2
D1 = [p, 1-p]
D2 = [p+e, 1-p-e]
f = p*log(p+e) + (1-p)*log(1-p-e)
arg min_e f = 0 -> e=0

Cross Entropy: Difference from D1 to D2
+ Sigma corresponding item
+ item = prob1 * tree_height2
+ tree_height2 = 1/prob2
-> CE = sigma -p1*log(p2)

