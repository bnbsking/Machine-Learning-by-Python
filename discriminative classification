#2D discriminative classification by gradient descent (Logistic regression)
#data set = training set + testing set. n=10. Neglect cross validation
#1. plot interation times - bias & variance
#2. plot prediction function and data set
#3. adjust to more complex model or regularization term

import matplotlib.pyplot as plt
import numpy as np
import math

#training set
Tra=[ [1.0, 2.0, 1], [1.0, 3.0, 1], [2.0, 4.0, 1], [3.0, 3.0, 1], [3.0, 5.0, 1],
     [2.5, 1.5, 0], [3.5, 2.0, 0], [4.5, 1.5, 0], [4.0, 1.0, 0], [5.0, 3.0, 0] ]
n=len(Tra)

#testing set
Tes=[ [1.5, 3.5, 1], [4.0, 2.5, 1], [3.5, 5.0, 1], [1.5, 6.0, 1], [2.5, 6.0, 1],
     [1.5, 1.0, 0], [2.5, 3.0, 0], [3.5, 3.0, 0], [4.5, 2.0, 0], [5.0, 4.0, 0] ]
n2=len(Tes)

X1=[]; Y1=[]; X2=[]; Y2=[];
for i in range(n):
    if Tra[i][2]==1:
        X1.append(Tra[i][0])
        Y1.append(Tra[i][1])
    else:
        X2.append(Tra[i][0])
        Y2.append(Tra[i][1])

X3=[]; Y3=[]; X4=[]; Y4=[];
for i in range(n):
    if Tes[i][2]==1:
        X3.append(Tes[i][0])
        Y3.append(Tes[i][1])
    else:
        X4.append(Tes[i][0])
        Y4.append(Tes[i][1])

plt.plot(X1,Y1,'ro',X2,Y2,'go',X3,Y3,'r^',X4,Y4,'g^')
plt.show()

def sig(b,w1,w2,x1,x2):
    return 1/(1+np.exp(-w1*x1-w2*x2-b))

def classify(b,w1,w2,x1,x2):
    if sig(b,w1,w2,x1,x2)>=0.5:
        return 1
    else:
        return 0

def loss(b,w1,w2):
    s=0
    for i in range(n):
        s=s-( (Tra[i][2])*math.log(sig(b,w1,w2,Tra[i][0],Tra[i][1]))
             +(1-(Tra[i][2]))*math.log(1-sig(b,w1,w2,Tra[i][0],Tra[i][0])) )
    return s
    
 b=0; w1=0; w2=0

D=0.0001; lr=0.001; it=1000
for i in range(it):
    b_=b; w1_=w1; w2_=w2
    b=b-lr*(loss(b_+D,w1_,w2_)-loss(b_,w1_,w2_))/D
    w1=w1-lr*(loss(b_,w1_+D,w2_)-loss(b_,w1_,w2_))/D
    w2=w2-lr*(loss(b_,w1_,w2_+D)-loss(b_,w1_,w2_))/D
    
#print(b,w1,w2)

bias=0
for i in range(n):
    if classify(b,w1,w2,Tra[i][0],Tra[i][1])!=Tra[i][2]:
        bias=bias+1
        print("classify error at Tra[{0}]".format(i))

var=0    
for i in range(n2):
    if classify(b,w1,w2,Tes[i][0],Tes[i][1])!=Tes[i][2]:
        var=var+1
        print("classify error at Tes[{0}]".format(i))
        
print("bias={0}".format(bias))
print("variance={0}".format(var))
print("accuracy={0}".format(1-var/n2))

pix = 256;
x = np.linspace(0.5, 5.5, pix)
y = np.linspace(0.5, 6.5, pix)
X,Y= np.meshgrid(x, y)
plt.xlabel("x"); plt.ylabel("y")
plt.plot(X1,Y1,'go',X2,Y2,'ro',X3,Y3,'g^',X4,Y4,'r^')
plt.contourf(X, Y, 1-sig(b,w1,w2,X,Y), 1, alpha=0.75, cmap=plt.cm.summer)
    #block color  (density, transparency, color)
C=plt.contour(X, Y, 1-sig(b,w1,w2,X,Y), 1, colors='black')
    #line color (density, color)
plt.clabel(C, inline=True, fontsize=10) #line color, number cover line?, fontsize
