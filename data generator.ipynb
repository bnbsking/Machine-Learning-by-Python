{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Reshape, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96845043 0.64812131 0.27250078 0.96509615 0.28636066 0.95594625\n",
      " 0.20458066 0.63759591 0.4749517  0.77248394] (10,)\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "if 0:\n",
    "    for i in range(6):\n",
    "        A = np.random.random((10,))\n",
    "        np.save('data/data{}.npy'.format(i), A)\n",
    "print(A, A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, n_classes, shuffle):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index:(index+1)]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        ID = list_IDs_temp[0]\n",
    "        X = np.load('data/' + ID + '.npy')\n",
    "        X = X[np.newaxis, :]\n",
    "        \n",
    "        y = np.zeros(self.n_classes)\n",
    "        y[self.labels[ID]] = 1\n",
    "        y = y[np.newaxis, :]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 3\n",
    "shuffle = True\n",
    "\n",
    "# Datasets\n",
    "partition = {'train': ['data0', 'data1', 'data2'], 'validation': ['data3', 'data4', 'data5']}\n",
    "labels = {'data0': 0, 'data1': 1, 'data2': 2, 'data3': 1, 'data4': 2, 'data5': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.94569562, 0.07265908, 0.44589503, 0.5849801 , 0.46483884,\n",
      "        0.52023664, 0.18430294, 0.16623775, 0.70766679, 0.63499653]]), array([[1., 0., 0.]])) (1, 10) (1, 3)\n",
      "(array([[0.84448407, 0.92908541, 0.90640278, 0.46929053, 0.72223718,\n",
      "        0.38170618, 0.64859279, 0.27141775, 0.57673044, 0.40825563]]), array([[0., 0., 1.]])) (1, 10) (1, 3)\n",
      "(array([[0.03280069, 0.46863122, 0.92635335, 0.51834472, 0.57592493,\n",
      "        0.49837597, 0.9402426 , 0.62870501, 0.5397904 , 0.64768882]]), array([[0., 1., 0.]])) (1, 10) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, n_classes, shuffle)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, n_classes, shuffle)\n",
    "\n",
    "for i in range(len(training_generator)):\n",
    "    item = training_generator.__getitem__(i)\n",
    "    print(item, item[0].shape, item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_164 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, input_shape=(10,)),\n",
    "    Activation('relu'),\n",
    "    Dense(5),\n",
    "    Activation('softmax'),\n",
    "    Dense(3),\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), \\\n",
    "    metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 1.4613 - categorical_crossentropy: 1.4613 - val_loss: 5.9058 - val_categorical_crossentropy: 5.9058\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 1.2193 - categorical_crossentropy: 1.2193 - val_loss: 5.9161 - val_categorical_crossentropy: 5.9161\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 1.0650 - categorical_crossentropy: 1.0650 - val_loss: 5.9179 - val_categorical_crossentropy: 5.9179\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.9852 - categorical_crossentropy: 0.9852 - val_loss: 5.9218 - val_categorical_crossentropy: 5.9218\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.9363 - categorical_crossentropy: 0.9363 - val_loss: 5.9307 - val_categorical_crossentropy: 5.9307\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
